##  Overview

*Data* is fundamental to all modern research and computation. It is the "currency" of the digital economy that envelops our modern society.

The formal processes for collecting, analyzing, storing and distributing this data are called *Data Management*.

These processes facilitate the discovery of useful information in the data, known as *Data Mining*, and coordinate the workflows and systems used in the extraction process, which are often implemented as a reproducible and reusable *Data Pipeline*.

These procedures also govern the data's structure and properties, called *Metadata*, as well as the monitoring and logging of the history of the data as it moves through the data pipeline, used in *Provenance*.

<!-- Data which has reached the "end" of the lifecycle will either be published for sharing and reuse, remain in storage for further analysis and processing, be archived for long-term storage or be deleted if it is deemed of no further value. -->

In order to fully grok the relationship between these components, it is important to start with an understanding of the *Data Lifecycle*.
